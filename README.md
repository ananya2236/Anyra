#  Anyra â€“ AI Voice Assistant

Anyra is an **AI-powered conversational voice assistant** that combines **real-time speech recognition, large language models, and text-to-speech synthesis** to create natural, human-like voice conversations.  
It features **session-based memory**, an **animated avatar**, and an interactive chat UI to make AI interactions more engaging and lifelike.

---

##  Features
- Live Voice Capture â€“ Supports **manual** & **automatic** recording modes.
- LLM Conversations â€“ Context-aware responses using **Google Gemini**.
- Natural Voice Output â€“ High-quality **Murf AI** text-to-speech.
- Fallback Voice â€“ Browser SpeechSynthesis if TTS fails.
- Animated AI Avatar â€“ Glowing orb & eye-tracking visuals.
- Dynamic Chat UI â€“ Switch between avatar mode and side-by-side chat history.
- Persistent Context â€“ Maintains session history using unique `session_id`.
- Audio Upload â€“ Save manual recordings locally for future reference.
- Error Handling â€“ Graceful fallback when APIs are unavailable.

---

## Tech Stack

| Layer            | Technology |
|------------------|------------|
| **Frontend**     | HTML, Tailwind CSS, JavaScript |
| **Backend**      | Python, FastAPI |
| **AI Model**     | Google Gemini 1.5 Flash |
| **Speech-to-Text** | AssemblyAI |
| **Text-to-Speech** | Murf API |
| **Styling**      | Tailwind CSS + custom animations |
| **Hosting**      | Local development (can be deployed on cloud) |

---

##  Architecture Overview

User speaks
â†“
Audio capture in browser (MediaRecorder API)
â†“
Send audio to FastAPI `/agent/chat/{session_id}`
â†“
AssemblyAI transcribes speech â†’ text
â†“
Gemini LLM generates contextual reply
â†“
Murf AI converts reply text â†’ audio
â†“
Browser plays audio + updates chat UI

## Installation & Setup

1ï¸. Clone the Repository
git clone https://github.com/your-username/anyra-ai-voice-assistant.git
cd anyra-ai-voice-assistant

2ï¸. Install Backend Dependencies
pip install -r requirements.txt

3ï¸. Create a .env File
MURF_API_KEY=your_murf_api_key
ASSEMBLYAI_API_KEY=your_assemblyai_api_key
GEMINI_API_KEY=your_gemini_api_key

4ï¸. Run the Backend Server
uvicorn app:app --reload


5. Backend will be available at:
http://127.0.0.1:8000

6. Serve the Frontend

You can open index.html directly via VS Code Live Server or:

python -m http.server 5500


7. Frontend will be available at:

http://127.0.0.1:5500/index.html

## Environment Variables

Variable	Description:

1.MURF_API_KEY	API key for Murf AI TTS
2.ASSEMBLYAI_API_KEY	API key for AssemblyAI STT
3.GEMINI_API_KEY	API key for Google Gemini LLM

## Usage

Start Backend â€“ Run uvicorn app:app --reload

Open Frontend â€“ Open index.html in your browser

Click Start â€“ Begin recording voice

Speak Naturally â€“ AI will transcribe, respond, and play audio

Switch Modes â€“ Use â€œShow Chatâ€ button to toggle chat view

Stop Recording â€“ Click â€œStopâ€ when done


## Screenshots

![alt text](image.png)


## Future Enhancements

ğŸŒ Multi-language support for STT & TTS

ğŸ“± Mobile-optimized UI

ğŸ’¾ Database-based conversation history

ğŸ”„ Continuous conversation without manual restarts


## Acknowledgments

Google Gemini â€“ LLM responses

Murf AI â€“ Text-to-Speech

AssemblyAI â€“ Speech-to-Text

FastAPI â€“ Backend framework

Tailwind CSS â€“ UI styling